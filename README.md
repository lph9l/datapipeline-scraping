# Pipeline de Scraping de Proyectos de Ley en LATAM

Este repositorio contiene un **pipeline de scraping** diseÃ±ado para extraer, procesar y almacenar de forma incremental **proyectos de ley publicados en portales oficiales** de LATAM (actualmente Colombia y PerÃº). El sistema es resiliente, escalable y fÃ¡cilmente configurable para incorporar nuevos paÃ­ses o portales.

## ðŸ“‹ Tabla de Contenidos

1. [DescripciÃ³n](#descripciÃ³n)
2. [CaracterÃ­sticas Principales](#caracterÃ­sticas-principales)
3. [Arquitectura y Componentes](#arquitectura-y-componentes)
4. [Estructura del Repositorio](#estructura-del-repositorio)
5. [Requisitos](#requisitos)
6. [ConfiguraciÃ³n](#configuraciÃ³n)
7. [InstalaciÃ³n y Despliegue](#instalaciÃ³n-y-despliegue)
8. [Uso del Pipeline](#uso-del-pipeline)
9. [Pruebas y Calidad de CÃ³digo](#pruebas-y-calidad-de-cÃ³digo)
10. [Contribuciones](#contribuciones)
11. [Licencia](#licencia)

---

## DescripciÃ³n

Este pipeline automatiza la **extracciÃ³n** de listados y detalles de proyectos de ley de portales oficiales, su **procesamiento** (limpieza, normalizaciÃ³n y clasificaciÃ³n), y su **almacenamiento** incremental en una base de datos PostgreSQL. AdemÃ¡s, enriquece cada proyecto con la clasificaciÃ³n por sector econÃ³mico mediante la **API de Gemini**.

## CaracterÃ­sticas Principales

- **Scrapers por PaÃ­s**: MÃ³dulos independientes para Colombia y PerÃº (configurables via YAML).
- **RotaciÃ³n de Proxies y Userâ€‘Agents**: Evita bloqueos y distribuye la carga de peticiones.
- **OrquestaciÃ³n con Airflow**: DefiniciÃ³n de DAGs modulares (`master_etl`, `scraping_etl`, `processing_etl`, `storage_etl`).
- **Enriquecimiento con API Gemini**: ClasificaciÃ³n por sector usando reglas locales y llamadas batch a Gemini.
- **Almacenamiento Incremental**: DetecciÃ³n de duplicados mediante checksum y upserts en tablas raw y final.
- **Logging y Monitoreo**: Logs en JSON, alertas en Slack, mÃ©tricas en Prometheus y dashboard en Grafana.
- **Pruebas Unitarias**: Cobertura >80% en mÃ³dulos crÃ­ticos, fixtures y mocks con pytest y respx.
- **Despliegue Dockerizado**: Contenedores Airflow y PostgreSQL coordinados con Docker Compose.

## Arquitectura y Componentes

1. **Scrapers** (`src/scrapers/`):
   - `ScraperBase` define la interfaz genÃ©rica.
   - `GenericListParser` y `GenericDetailParser` para parseo configurable.
   - `DynamicScraper` orquesta fetch, parse y control de duplicados.
2. **Proxy Manager** (`src/scrapers/network/`):
   - GestiÃ³n y validaciÃ³n de proxies gratuitos.
   - RotaciÃ³n de Userâ€‘Agents y lÃ³gica de retries con tenacity.
3. **OrquestaciÃ³n (Airflow)** (`dags/`):
   - DAGs modulares para scraping, procesamiento y almacenamiento.
   - Tareas con PythonOperator, TaskGroup y Branching.
4. **ClasificaciÃ³n** (`src/classifier.py`):
   - Reglas locales + API Gemini.
   - Batch processing y validaciÃ³n de categorÃ­as.
5. **Almacenamiento** (`src/storage.py`):
   - Adaptadores PostgreSQL.
   - Upserts incremental en tablas `raw_projects` y `final_projects`.
6. **ConfiguraciÃ³n** (`configs/`):
   - YAML por paÃ­s (`colombia.yml`, `peru.yml`) y `classifier.yml`.
7. **Pruebas** (`test/`):
   - pytest, mocks con respx, fixtures HTML.

## Estructura del Repositorio

```plain
â”œâ”€â”€ dags/                     # DefiniciÃ³n de DAGs de Airflow
â”œâ”€â”€ configs/                  # Archivos YAML de configuraciÃ³n
â”‚   â”œâ”€â”€ colombia.yml
â”‚   â”œâ”€â”€ peru.yml
â”‚   â””â”€â”€ classifier.yml
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ scrapers/             # Implementaciones de ScraperBase
â”‚       â”œâ”€â”€ network/          # Proxy Manager y HTTP Client
â”‚   â”œâ”€â”€ classifier.py         # LÃ³gica de clasificaciÃ³n sectorial
â”‚   â””â”€â”€ storage.py            # Adaptadores y funciones de upsert
â”œâ”€â”€ test/                     # Pruebas unitarias y fixtures
â”œâ”€â”€ Dockerfile                # DefiniciÃ³n de imagen base Airflow + pipeline
â”œâ”€â”€ docker-compose.yml        # OrquestaciÃ³n de servicios
â”œâ”€â”€ Makefile                  # Tareas comunes (build, up, etc.)
â”œâ”€â”€ requirements.txt          # Dependencias Python
â””â”€â”€ README.md                 # Este documento
```

## Requisitos

- Docker â‰¥ 20.10 y Docker Compose â‰¥ 1.29
- Make
- Python 3.9 (para desarrollo local)
- Variables de entorno definidas en `.env`

## ConfiguraciÃ³n

1. Clona el repositorio:
   ```bash
   git clone https://github.com/lph9l/scraping-datapipeline.git
   cd scraping-datapipeline
   ```
2. Copia el ejemplo de variables:
   ```bash
   cp .env.example .env
   ```
3. Edita `.env` con:
   ```dotenv
   COUNTRY=colombia        # o "peru"
   GEMINI_API_KEY=<tu_api_key>
   ```

## InstalaciÃ³n y Despliegue

Construye la imagen y levanta los servicios:
```bash
make build      # Construye con Dockerfile
make up         # Levanta Airflow y Postgres con docker-compose
```
En desarrollo local, tambiÃ©n puedes:
```bash
docker-compose run --rm webserver airflow db init
docker-compose run --rm webserver \
  airflow users create \
    --username admin \
    --password admin \
    --firstname Admin \
    --lastname User \
    --role Admin \
    --email admin@example.com
```

## Uso del Pipeline

1. Accede a la interfaz de Airflow en `http://localhost:8080`.
2. Crea una variable de entorno de Airflow con:
   ```dotenv
   COUNTRY=colombia        # o "peru"
   ```
3. Trigger manual del DAG `master_etl` o espera su ejecuciÃ³n programada.
4. Revisa logs, mÃ©tricas en Airflow y estado de los dags.
5. Consulta datos almacenados en PostgreSQL via `psql` o pgAdmin.

## Pruebas y Calidad de CÃ³digo

Ejecuta el suite de tests:
```bash
pytest -q
```
Se utilizan:
- **pytest** para pruebas unitarias.
- **respx** y fixtures HTML para mockear respuestas HTTP.
- **flake8**, **black** e **isort** integrados en pre-commit.

## Contribuciones

1. Haz un fork del proyecto.
2. Crea una rama (`git checkout -b feature/nueva-funcionalidad`).
3. Realiza tus cambios y agrega pruebas.
4. AsegÃºrate de pasar los tests y linters.
5. Abre un Pull Request describiendo tu aporte.

## Licencia

Este proyecto estÃ¡ bajo la [Licencia MIT](LICENSE).

---

